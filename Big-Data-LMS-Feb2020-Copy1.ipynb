{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required dataset and transform it into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/danims/Documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + \"/acs2017_census_tract_data.csv\"\n",
    "df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(file_path)\n",
    "# can add inferSchema=True to avoid loading as trings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of columns and rows for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some of the libraries and functions needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import column\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping variables not required for the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = reduce(DataFrame.drop, ['MeanCommute','Walk','Transit','VotingAgeCitizen',\\\n",
    "                              'Income','IncomeErr','IncomePerCap','IncomePerCapErr',\\\n",
    "                             'Professional','Service','Office','Construction',\\\n",
    "                              'Production','Drive','Carpool','OtherTransp','WorkAtHome',\\\n",
    "                             'Employed', 'PrivateWork','PublicWork',\\\n",
    "                              'SelfEmployed','FamilyWork', 'Unemployment'], df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataframe column names and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of null values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.types import DecimalType, StructType, StructField\n",
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replaces null values with value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the data type in numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"TractId\", df2[\"TractId\"].cast(IntegerType()))\n",
    "df2 = df2.withColumn(\"TotalPop\", df2[\"TotalPop\"].cast(IntegerType()))\n",
    "df2 = df2.withColumn(\"Men\", df2[\"Men\"].cast(IntegerType()))\n",
    "df2 = df2.withColumn(\"Women\", df2[\"Women\"].cast(IntegerType()))\n",
    "df2 = df2.withColumn(\"Hispanic\", df2[\"Hispanic\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"White\", df2[\"White\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"Black\", df2[\"Black\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"Native\", df2[\"Native\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"Asian\", df2[\"Asian\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"Pacific\", df2[\"Pacific\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"Poverty\", df2[\"Poverty\"].cast(FloatType()))\n",
    "df2 = df2.withColumn(\"ChildPoverty\", df2[\"ChildPoverty\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(['TotalPop','Men','Asian','Black']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and inserting a column with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"Others\", 100 - f.round(sum(df2[\"Hispanic\",\"White\",\"Black\",\"Native\",\"Asian\",\"Pacific\"]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do fopr all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(['TotalPop','Asian','Black','Others']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting percentage into values with decimal points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "df = sqlContext.sql(\"SELECT TractId,State, County,TotalPop,Men,Women,\\\n",
    "                         floor(round((Hispanic * TotalPop)/100,0)) as Hispanic, \\\n",
    "                         floor(round((White * TotalPop)/100,0)) as White,\\\n",
    "                         floor(round((Black * TotalPop)/100,0)) as Black, \\\n",
    "                         floor(round((Asian * TotalPop)/100,0)) as Asian, \\\n",
    "                         floor(round((Native * TotalPop)/100,0)) as Native, \\\n",
    "                         floor(round((Pacific * TotalPop)/100,0)) as Pacific,\\\n",
    "                         floor(round((Others * TotalPop)/100,0)) as Others,\\\n",
    "                         floor(round((Poverty * TotalPop)/100,0)) as Poverty, \\\n",
    "                         floor(round((ChildPoverty * TotalPop)/100,0)) as ChildPoverty\\\n",
    "                         FROM States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"States_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = sqlContext.sql(\"SELECT State, County,sum(TotalPop) as TotalPop,sum(Men) as Men,sum(Women) as Women, sum(Hispanic) as Hispanic,\\\n",
    "                         sum(White) as White, sum(Black) as Black, sum(Asian) as Asian, sum(Native) as Native,sum(Pacific) as Pacific,\\\n",
    "                         sum(Others) as Others,sum(Poverty) as Poverty,sum(ChildPoverty) as ChildPoverty\\\n",
    "                         FROM States_a group by State, County\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting dataframes into csv to be use for visualiation and model output analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.format('csv').save('/home/danims/Documents/census-data1',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.coalesce(1).write.format('csv').save('/home/danims/Documents/census-data2',header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    " \n",
    "#df = sqlCtx.read.format('com.databricks.spark.csv').option('header', 'true').option('inferschema', 'true').load('corr_test.csv')\n",
    "#dfa = datos\n",
    "col_names = dfa.columns\n",
    "features = dfa.rdd.map(lambda row: row[0:])\n",
    "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols= ['Hispanic','White','Black','Native','Asian','Pacific', 'Others'], outputCol=vector_col)\n",
    "df_vector = assembler.transform(dfa).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sql = spark.sql ( \"SELECT State, max(TotalPop), round(avg(TotalPop),2) as Avg , max(TotalPop) \\\n",
    "                      FROM states \\\n",
    "                      GROUP BY State \\\n",
    "                      ORDER BY Count(*) DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression:useful when you have data in which a predictions about one variable can be made using the knowlegde about another varaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssemble = VectorAssembler(inputCols = ['Hispanic','White','Black','Native','Asian','Pacific', 'Others'], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = vectorAssemble.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = \"features\", labelCol = \"Poverty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls lr1.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(['Hispanic','White','Black','Native','Asian']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(['Pacific', 'Others','Poverty']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize=(100, 100)\n",
    "values = result_pdf['TotalPop']\n",
    "names = result_pdf['State']\n",
    "plt.bar(names, values)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAsseDTR = VectorAssembler(inputCols = ['Hispanic','White','Black','Native','Asian','Pacific', 'Others'], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtr = vectorAsseDTR.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df_dtr.randomSplit([0.7,0.3])\n",
    "train_dtr = splits[0]\n",
    "test_dtr = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dtr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(featuresCol = \"features\", labelCol = \"Poverty\")\n",
    "dtr_model = dtr.fit(train_dtr)\n",
    "dtr_pred = dtr_model.transform(test_dtr)\n",
    "dtr_eval = RegressionEvaluator(labelCol = \"Poverty\" , predictionCol = \"prediction\" , metricName = \"rmse\")\n",
    "rmse = dtr_eval.evaluate(dtr_pred)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_model = dtr.fit(train_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_pred = dtr_model.transform(test_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_eval = RegressionEvaluator(labelCol = \"Poverty\" , predictionCol = \"prediction\" , metricName = \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = dtr_eval.evaluate(dtr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = \"features\" , labelCol = \"Poverty\")\n",
    "gbt_model = gbt.fit(train_dtr)\n",
    "gbt_pred = gbt_model.transform(test_dtr)\n",
    "gbt_eval = RegressionEvaluator(labelCol = \"Poverty\", predictionCol = \"prediction\", metricName = \"rmse\")\n",
    "gbt_rmse = gbt_eval.evaluate(gbt_pred)\n",
    "gbt_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol = \"features\" , labelCol = \"Poverty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model = gbt.fit(train_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pred = gbt_model.transform(test_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_eval = RegressionEvaluator(labelCol = \"Poverty\", predictionCol = \"prediction\", metricName = \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_rmse = gbt_eval.evaluate(gbt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
